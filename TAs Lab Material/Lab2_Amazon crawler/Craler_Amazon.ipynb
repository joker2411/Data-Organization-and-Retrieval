{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7fda07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f163c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# images and text\n",
    "\n",
    "no_pages = 3\n",
    "imagedownloadLinks=[]\n",
    "def get_data(pageNo): \n",
    "\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    #proxy = '103.15.60.23:8080'\n",
    "    r = requests.get('https://www.amazon.in/s?bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031&dc&qid=1631382284&rnid=1389401031&ref=lp_1389401031_nr_n_1',headers=headers,timeout=600)#proxies = {'http':proxy,'https':proxy},timeout=600)\n",
    "    #r = requests.get('https://www.amazon.in/s?bbn=1389432031&rh=n%3A1389432031%2Cp_36%3A1318504031&dc&qid=1631548930&rnid=1318502031&ref=lp_1389432031_nr_p_36_0',headers=headers,timeout=600)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content)\n",
    "    #print(soup)\n",
    "    alls = []\n",
    "    counter=0;\n",
    "    \n",
    "    for d in soup.findAll('div', attrs={'class':'a-column a-span4'}):\n",
    "        #print(d)\n",
    "        #break\n",
    "        name = d.find('span', attrs={'class':'a-size-base-plus a-color-base a-text-normal'})\n",
    "        price = d.find('span', attrs={'class':'a-price-whole'})\n",
    "        olderprice=d.find('span', attrs={'class':'a-offscreen'})\n",
    "        \n",
    "        #Image scraping from webpage\n",
    "        imageURL2=d.find('img',attrs={'class':'s-image'})\n",
    "        #print(imageURL2)\n",
    "        imageURL=imageURL2['src']\n",
    "        #imageName=imageURL2['alt']\n",
    "        all1=[]\n",
    "        if name is not None:\n",
    "            name1=name.text\n",
    "            name1=name1.strip()\n",
    "            all1.append(name1)\n",
    "        else:\n",
    "            all1.append(\"unknown-product\")\n",
    "            \n",
    "        if price is not None:\n",
    "            all1.append(price.text)\n",
    "        else:\n",
    "            all1.append(\"unknown-price\")\n",
    "        \n",
    "        if olderprice is not None:\n",
    "            olderprice1=olderprice.text\n",
    "            olderprice1=olderprice1.strip()\n",
    "            all1.append(olderprice1)\n",
    "        else:\n",
    "            all1.append(\"unknown-olderprice\")\n",
    "        if imageURL is not None:\n",
    "            all1.append(imageURL)    \n",
    "        else:\n",
    "            all1.append(\"unknown-URL\")\n",
    "            \n",
    "        alls.append(all1)    \n",
    "    return alls\n",
    "\n",
    "#Calling program for images and text\n",
    "results = []\n",
    "for i in range(1, no_pages+1):\n",
    "    results.append(get_data(i))\n",
    "flatten = lambda l: [item for sublist in l for item in sublist] #flattens a list of lists to produce a single list\n",
    "df = pd.DataFrame(flatten(results),columns=['Mobile Name','Price','Old_Price','URL'])\n",
    "df.to_csv('C:/Users/Hp/Desktop/ResearchWork/LabWork/amazon_products.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d847899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images... \n"
     ]
    }
   ],
   "source": [
    "def get_images(pageNo): \n",
    "\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    r = requests.get('https://www.amazon.in/s?bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031&dc&qid=1631382284&rnid=1389401031&ref=lp_1389401031_nr_n_1',headers=headers,timeout=1500)#proxies = {'http':proxy,'https':proxy},timeout=600)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content)\n",
    "    #print(soup)\n",
    "    alls = []\n",
    "    counter=0;\n",
    "    \n",
    "    for d in soup.findAll('div', attrs={'class':'a-column a-span4'}):\n",
    "        #Image scraping from webpage\n",
    "        imageURL2=d.find('img',attrs={'class':'s-image'})\n",
    "        imageURL=imageURL2['src']\n",
    "        #print(imageURL)\n",
    "        if imageURL is not None: \n",
    "            counter=counter+1\n",
    "            urllib.request.urlretrieve(imageURL, 'C:/Users/Hp/Desktop/ResearchWork/LabWork/Amazon_Images/'+str(counter)+'.jpg')\n",
    "            #C:\\Users\\Hp\\Desktop\\ResearchWork\\LabWork\\Amazon_Images\n",
    "############################Calling function to scrap images#####################################################################################################\n",
    "\n",
    "results = []\n",
    "no_pages = 3\n",
    "print(\"Getting images... \")\n",
    "get_images(no_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos\n",
    "import urllib.request\n",
    "def get_video_data(): \n",
    "\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    #proxy = '103.15.60.23:8080'\n",
    "    r = requests.get('https://research.google.com/youtube8m/explore.html',headers=headers,timeout=600)#proxies = {'http':proxy,'https':proxy},timeout=600)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content)\n",
    "    #print(soup)\n",
    "    alls = []\n",
    "    counter=0;\n",
    "    \n",
    "    for d in soup.findAll('div', attrs={'class':'contents'}):\n",
    "        #Image scraping from webpage\n",
    "#       imageURL2=d.find('img')\n",
    "        #imageURL=imageURL2['src']\n",
    "        #imageName=imageURL2['alt'\n",
    "        print(d)\n",
    "        videoURL2 = d.find('a')\n",
    "        #videoURL=videoURL2['href']\n",
    "        print(videoURL2)\n",
    "        #videoURL2=d.find('video')\n",
    "        #videoURL2=videoURL['href']\n",
    "        #imageName=imageURL2['alt']\n",
    "#         if(counter<=5):\n",
    "#             if videoURL is not None:\n",
    "#                 print(videoURL) \n",
    "#                 counter=counter+1\n",
    "#                 urllib.request.urlretrieve(videoURL, \"C:/Users/Hp/Desktop/ResearchWork/LabWork/Amazon_Images/videos\"+str(counter))\n",
    "\n",
    "\n",
    "#calling program for video\n",
    "get_video_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99178da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmdissue23",
   "language": "python",
   "name": "pmdissue23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
