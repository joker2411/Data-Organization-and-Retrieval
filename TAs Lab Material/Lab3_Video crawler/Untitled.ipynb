{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadebe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to CSV file...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "\n",
    "#################################################################################################################################\n",
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "################################################################################################################################\n",
    "\n",
    "\n",
    "tkWindow = Tk()  \n",
    "tkWindow.configure(background='#CCCCFF')\n",
    "tkWindow.geometry('400x250')  \n",
    "tkWindow.title('Crawler for Text, Image and Video')\n",
    "cwd = os.getcwd()\n",
    "\n",
    "########################################Function to scrap text data#########################################################################################\n",
    "\n",
    "no_pages = 2\n",
    "imagedownloadLinks=[]\n",
    "def get_data(pageNo): \n",
    "\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    #proxy = '103.15.60.23:8080'\n",
    "    r = requests.get('https://www.amazon.in/s?bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031&dc&qid=1631382284&rnid=1389401031&ref=lp_1389401031_nr_n_1',headers=headers,timeout=1500)#proxies = {'http':proxy,'https':proxy},timeout=600)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content, features=\"html.parser\")\n",
    "    #print(soup)\n",
    "    alls = []\n",
    "    counter=0;\n",
    "    \n",
    "    for d in soup.findAll('div', attrs={'class':'a-section a-spacing-none octopus-pc-item-block octopus-pc-asin-block'}):\n",
    "        name = d.find('span', attrs={'class':'a-size-base a-color-base'})\n",
    "        price = d.find('span', attrs={'class':'a-price-whole'})\n",
    "        olderprice=d.find('span', attrs={'class':'a-size-mini a-color-tertiary a-text-strike'})\n",
    "        \n",
    "        #Image scraping from webpage\n",
    "        imageURL2=d.find('img')\n",
    "        imageURL=imageURL2['src']\n",
    "        all1=[]\n",
    "        if name is not None:\n",
    "            name1=name.text\n",
    "            name1=name1.strip()\n",
    "            all1.append(name1)\n",
    "        else:\n",
    "            all1.append(\"unknown-product\")\n",
    "            \n",
    "        if price is not None:\n",
    "            all1.append(price.text)\n",
    "        else:\n",
    "            all1.append(\"unknown-price\")\n",
    "        \n",
    "        if olderprice is not None:\n",
    "            olderprice1=olderprice.text\n",
    "            olderprice1=olderprice1.strip()\n",
    "            all1.append(olderprice1)\n",
    "        else:\n",
    "            all1.append(\"unknown-olderprice\")\n",
    "            \n",
    "        if imageURL is not None:\n",
    "            all1.append(imageURL)\n",
    "        else:\n",
    "            all1.append(\"unknown-URL\")\n",
    "            \n",
    "        alls.append(all1)    \n",
    "    \n",
    "    return alls\n",
    "#######################################Function to scrap images##########################################################################################\n",
    "\n",
    "imagedownloadLinks=[]\n",
    "def get_images(pageNo): \n",
    "\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    r = requests.get('https://www.amazon.in/s?bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031&dc&qid=1631382284&rnid=1389401031&ref=lp_1389401031_nr_n_1',headers=headers,timeout=1500)#proxies = {'http':proxy,'https':proxy},timeout=600)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content,features=\"html.parser\")\n",
    "    #print(soup)\n",
    "    alls = []\n",
    "    counter=0;\n",
    "    \n",
    "    for d in soup.findAll('div', attrs={'class':'a-section a-spacing-none octopus-pc-item-block octopus-pc-asin-block'}):\n",
    "        #Image scraping from webpage\n",
    "        imageURL2=d.find('img')\n",
    "        imageURL=imageURL2['src']\n",
    "\t\t\n",
    "        if imageURL is not None: \n",
    "            counter=counter+1\n",
    "            urllib.request.urlretrieve(imageURL, cwd+\"/Images/\"+\"Image \"+\"_\"+str(counter)+\".jpg\")\n",
    "            \n",
    "############################Calling function to scrap images#####################################################################################################\n",
    "def amazonImage():\n",
    "    results = []\n",
    "    no_pages = 2\n",
    "    print(\"Getting images... \")\n",
    "    get_images(no_pages)\n",
    "#############################Calling function to scrap text data####################################################################################################\n",
    "\n",
    "def amazonData():\n",
    "    results = []\n",
    "    print(\"Writing data to CSV file...\")\n",
    "    for i in range(1, no_pages+1):\n",
    "    \tresults.append(get_data(i))\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist] #flattens a list of lists to produce a single list\n",
    "    df = pd.DataFrame(flatten(results),columns=['Mobile Name','Price','Old Price','URL'])\t\n",
    "    df.to_csv(cwd+'/amazon_products.csv', index=True, encoding='utf-8')\n",
    "\n",
    "####################################Function to download video#############################################################################################\n",
    "\n",
    "\n",
    "def get_video_data(): \n",
    "\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    #proxy = '103.15.60.23:8080'\n",
    "    r = requests.get('https://navbharattimes.indiatimes.com/video/news/kashmiri-boy-funny-reaction-on-injection-watch-viral-video/videoshow/86226291.cms',headers=headers,timeout=600)#proxies = {'http':proxy,'https':proxy},timeout=600)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content,\"html.parser\")\n",
    "    #print(soup)\n",
    "    alls = []\n",
    "    counter=0;\n",
    "    \n",
    "    for d in soup.findAll(\"meta\", itemprop=\"contenturl\"):\n",
    "        file_name = d[\"content\"].split('/')[-1] \n",
    "        \n",
    "        print( \"Downloading file:%s\"%file_name)\n",
    "        r2 = requests.get(d[\"content\"], stream = True)\n",
    "        with open(file_name, 'wb') as f: \n",
    "            for chunk in r2.iter_content(chunk_size = 1024*1024): \n",
    "                if chunk: \n",
    "                    f.write(chunk) \n",
    "          \n",
    "        print( \"%s downloaded!\\n\"%file_name )\n",
    "        print (\"All videos downloaded!\")\n",
    "        #counter=counter+1\n",
    "        #urllib.request.urlretrieve(d[\"content\"], \"C:/Users/Hp/Desktop/ResearchWork/LabWork/Amazon_Images/videos\"+str(counter))\n",
    "    \n",
    "\n",
    "########################################Buttons on UI################################################################################################\n",
    "\n",
    "\n",
    "b1 = Button(tkWindow,\n",
    "\ttext = 'Amazon Mobile Models Data',\n",
    "\tactiveforeground ='#0099ff',\n",
    "\tbd = '5',width=30,\n",
    "\tcommand = amazonData)\n",
    "\t\n",
    "  \n",
    "\n",
    "b2 = Button(tkWindow,\n",
    "\ttext = 'Amazon Mobile Models Images',\n",
    "\tactiveforeground ='#0099ff',\n",
    "\tbd = '5',width=25,\n",
    "\tcommand = amazonImage)  \n",
    "\n",
    "\n",
    "\n",
    "b3 = Button(tkWindow,\n",
    "\ttext = 'Viral Vedio',\n",
    "\tactiveforeground ='#0099ff',\n",
    "\tbd = '5',width=25,\n",
    "\tcommand = get_video_data)  \n",
    "\n",
    "\n",
    "b4 = Button(tkWindow,\n",
    "\ttext = 'Exit',\n",
    "\tactiveforeground ='#0099ff',\n",
    "\tbd = '1',width=20,\n",
    "\tcommand = exit)\n",
    "b1.pack(side = TOP)  \n",
    "  \n",
    "b2.pack(side = LEFT)  \n",
    "  \n",
    "b3.pack(side = RIGHT)  \n",
    "  \n",
    "b4.pack(side = BOTTOM)  \n",
    "tkWindow.mainloop()\n",
    "\n",
    "\n",
    "#########################################################################################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe860ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
